{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk0qUhJUQrkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd452d3-5d37-435e-8bce-8462e27ab6f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.9/221.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4qBIihE4Rpq"
      },
      "outputs": [],
      "source": [
        "# LangChain components to use\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Support for dataset retrieval with Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
        "# you will also initialize the DB connection:\n",
        "import cassio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIs76OPQ6JyD",
        "outputId": "08add583-5c78-4911-9363-1cb243f185a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/232.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "# import pinecone\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "h4qNPzoJ2Lyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "1itBNL1v6N9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqpM6I854Rpr"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:YvjAbOmYuolqNFMKKXwsZUzM:69e3793fdae42466057bf697f5937b4c3e6fe5601c160ed32b38b0d368d10550\" # enter the \"AstraCS:...\" string found in in your Token JSON file\n",
        "ASTRA_DB_ID = \"1488ccc6-113b-46e5-ba7c-41cbb6e7cc3f\"\n",
        "\n",
        "OPENAI_API_KEY = \"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\" # enter your OpenAI key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "Ngi14RIC5HsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assistant(docs):\n",
        "    loader = PyPDFLoader(docs)\n",
        "    documents = loader.load()\n",
        "    # text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    # doc = text_splitter.split_documents(documents)\n",
        "    return documents"
      ],
      "metadata": {
        "id": "mm4Oag7t4wnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nyWsJYH546O",
        "outputId": "c9ae73c1-e00b-4904-82f5-0c121e56923a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.2-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.9/277.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = assistant('/content/7-first-shift-paper.pdf')"
      ],
      "metadata": {
        "id": "hwUGdrr148LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# provide the path of  pdf file/files.\n",
        "# pdfreader = PdfReader('/content/7-first-shift-paper.pdf')"
      ],
      "metadata": {
        "id": "waVKJW-n6jqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW17tKFb1HNX",
        "outputId": "70133535-a345-4107-c0ef-aa9e32282070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from typing_extensions import Concatenate\n",
        "# # read text from pdf\n",
        "# raw_text = ''\n",
        "# for i, page in enumerate(pdfreader.pages):\n",
        "#     content = page.extract_text()\n",
        "#     if content:\n",
        "#         raw_text += content"
      ],
      "metadata": {
        "id": "42BKuFRO6meP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken -q"
      ],
      "metadata": {
        "id": "vR41Iq-4ZHnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFBR5HnZSPmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd6c65c-85ef-46ca-8109-80fad3a0a2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 1488ccc6-113b-46e5-ba7c-41cbb6e7cc3f-us-east-2.db.astra.datastax.com:29042:36887b67-fd8a-4452-9d77-6c6fe16ea1b1. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 1488ccc6-113b-46e5-ba7c-41cbb6e7cc3f-us-east-2.db.astra.datastax.com:29042:36887b67-fd8a-4452-9d77-6c6fe16ea1b1. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(140319039386480) 1488ccc6-113b-46e5-ba7c-41cbb6e7cc3f-us-east-2.db.astra.datastax.com:29042:36887b67-fd8a-4452-9d77-6c6fe16ea1b1> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 1488ccc6-113b-46e5-ba7c-41cbb6e7cc3f-us-east-2.db.astra.datastax.com:29042:36887b67-fd8a-4452-9d77-6c6fe16ea1b1. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ],
      "source": [
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TavS0AK2SLrL"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg9VAk4USQvU"
      },
      "outputs": [],
      "source": [
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"qa_mini_demo\",\n",
        "    session=None,\n",
        "    keyspace=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.text_splitter import CharacterTextSplitter\n",
        "# # We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "# text_splitter = CharacterTextSplitter(\n",
        "#     separator = \"\\n\",\n",
        "#     chunk_size = 800,\n",
        "#     chunk_overlap  = 200,\n",
        "#     length_function = len,\n",
        "# )\n",
        "# texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "9FMAhKr77AVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX5BECsdSUUM",
        "outputId": "43c361f1-7ee7-41da-ec6b-0cc9f134ecfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 50 headlines.\n"
          ]
        }
      ],
      "source": [
        "astra_vector_store.add_texts(texts[:50])\n",
        "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbJugrh7SX3C"
      },
      "outputs": [],
      "source": [
        "# first_question = True\n",
        "# while True:\n",
        "#     if first_question:\n",
        "#         query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
        "#     else:\n",
        "#         query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "#     if query_text.lower() == \"quit\":\n",
        "#         break\n",
        "\n",
        "#     if query_text == \"\":\n",
        "#         continue\n",
        "\n",
        "#     first_question = False\n",
        "\n",
        "#     print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
        "#     answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
        "#     print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
        "\n",
        "#     print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "#     for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
        "#         print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similiar_docs(query, k=2, score=False):\n",
        "  if score:\n",
        "    similar_docs = index.similarity_search_with_score(query, k=k)\n",
        "  else:\n",
        "    similar_docs = index.similarity_search(query, k=k)\n",
        "  return similar_docs\n"
      ],
      "metadata": {
        "id": "dSaUPguw389l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"text-davinci-003\"\n",
        "# model_name = \"gpt-3.5-turbo\"\n",
        "model_name = \"gpt-4\"\n",
        "llm = OpenAI(model_name=model_name)\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "def get_answer(query):\n",
        "  similar_docs = get_similiar_docs(query)\n",
        "  answer = chain.run(input_documents=similar_docs, question=query)\n",
        "  return answer\n"
      ],
      "metadata": {
        "id": "WmTXaLmN6tOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\n",
        "answer = get_answer(query)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "nZFmJGSG6wB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\n",
        "answer = get_answer(query)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "EJhfylUy6zRY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}