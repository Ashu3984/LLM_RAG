{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbqGr7QasE0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "rFAoBNInZEZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6iBS3GUY73e"
      },
      "outputs": [],
      "source": [
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "hrKD__5nZJD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_PROJECT=combined_bengali_lora_odiagen-v0"
      ],
      "metadata": {
        "id": "rO-qluqGZMF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "id": "epR3ovp_ZSaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "\n",
        "assert (\n",
        "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
        "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from peft import (\n",
        "    prepare_model_for_int8_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    set_peft_model_state_dict\n",
        ")"
      ],
      "metadata": {
        "id": "8wLZVeTqZZVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimized for RTX 4090. for larger GPUs, increase some of these?\n",
        "MICRO_BATCH_SIZE = 32  # this could actually be 5 but i like powers of 2\n",
        "BATCH_SIZE = 64\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 6  # we don't always need 3 tbh\n",
        "LEARNING_RATE = 3e-4  # the Karpathy constant\n",
        "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "VAL_SET_SIZE = 2000\n",
        "TARGET_MODULES = [\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"down_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "]\n",
        "train_on_inputs = True\n",
        "add_eos_token = False\n",
        "\n",
        "# DATA_PATH = \"/content/odia_alpaca_qa_data.json\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/odia_checkpoint/A100_combined_lora_odia_v1\""
      ],
      "metadata": {
        "id": "3x2hk8fgZfIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_map = \"auto\"\n",
        "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "ddp = world_size != 1\n",
        "if ddp:\n",
        "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
        "    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size"
      ],
      "metadata": {
        "id": "z-gGmX7rZkIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OERIU_4XZoBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "# import torch\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "P9ysLm9pZoND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=device_map,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\"\n",
        ")\n",
        "\n",
        "tokenizer.pad_token_id = (\n",
        "        0  # unk. we want this to be different from the eos token\n",
        ")\n",
        "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
        "\n",
        "model = prepare_model_for_int8_training(model)"
      ],
      "metadata": {
        "id": "I8YkFH5QZzKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "# tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "data = load_dataset(\"OdiaGenAI/all_combined_bengali_252k\")"
      ],
      "metadata": {
        "id": "ZTJ0nqB7Z2J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()  # Be more transparent about the % of trainable params."
      ],
      "metadata": {
        "id": "ZoVli7pnZ9Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(instruction, input, label):\n",
        "\n",
        "  if input:\n",
        "    res = f\"\"\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\"\"\n",
        "  else:\n",
        "    res = f\"\"\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\"\"\n",
        "\n",
        "  if label:\n",
        "    res = f\"{res}{label}\"\n",
        "\n",
        "  return res\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(prompt, add_eos_token=True):\n",
        "  # there's probably a way to do this with the tokenizer settings\n",
        "  # but again, gotta move fast\n",
        "  result = tokenizer(\n",
        "      prompt,\n",
        "      truncation=True,\n",
        "      max_length=CUTOFF_LEN,\n",
        "      padding=False,\n",
        "      return_tensors=None,\n",
        "  )\n",
        "  if (\n",
        "      result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "      and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
        "      and add_eos_token\n",
        "  ):\n",
        "      result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "      result[\"attention_mask\"].append(1)\n",
        "\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "cNYR4f1ZaLUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_tokenize_prompt(data_point):\n",
        "  full_prompt = generate_prompt(\n",
        "      data_point[\"instruction\"],\n",
        "      data_point[\"input\"],\n",
        "      data_point[\"output\"],\n",
        "  )\n",
        "  tokenized_full_prompt = tokenize(full_prompt)\n",
        "  if not train_on_inputs:\n",
        "      user_prompt = generate_prompt(\n",
        "          data_point[\"instruction\"], data_point[\"input\"]\n",
        "      )\n",
        "      tokenized_user_prompt = tokenize(\n",
        "          user_prompt, add_eos_token=add_eos_token\n",
        "      )\n",
        "      user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
        "\n",
        "      if add_eos_token:\n",
        "          user_prompt_len -= 1\n",
        "\n",
        "      tokenized_full_prompt[\"labels\"] = [\n",
        "          -100\n",
        "      ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
        "          user_prompt_len:\n",
        "      ]  # could be sped up, probably\n",
        "  return tokenized_full_prompt"
      ],
      "metadata": {
        "id": "0Tue01qcaUe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qOiQTLBUaXrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data.remove_columns(\"data_source\")"
      ],
      "metadata": {
        "id": "fVmqzCZaaX2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "id": "UqdfBzogaYIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if VAL_SET_SIZE > 0:\n",
        "    train_val = dataset[\"train\"].train_test_split(\n",
        "        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
        "    )\n",
        "    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "else:\n",
        "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "    val_data = None"
      ],
      "metadata": {
        "id": "dHmFs2KgadP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "KzbYr5GragYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data"
      ],
      "metadata": {
        "id": "NSYuhOl6agzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m87ndUNRai3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_torch\",\n",
        "        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=100 if VAL_SET_SIZE > 0 else None,\n",
        "        save_steps=100,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        save_total_limit=3,\n",
        "        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n",
        "        ddp_find_unused_parameters=False if ddp else None,\n",
        "        group_by_length=False,\n",
        "        report_to=\"wandb\",  # enable logging to W&B\n",
        "        run_name=\"combine_lora_bengali_v0\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
        "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        "        ),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "old_state_dict = model.state_dict\n",
        "model.state_dict = (\n",
        "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
        ").__get__(model, type(model))\n",
        "\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)"
      ],
      "metadata": {
        "id": "HIMbmOVhajJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.autocast(\"cuda\"):\n",
        "  trainer.train()"
      ],
      "metadata": {
        "id": "bXIQsbtVaw0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/\")"
      ],
      "metadata": {
        "id": "uGD5e_sDa7Uh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}